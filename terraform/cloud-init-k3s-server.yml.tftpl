#cloud-config
hostname: ${hostname}
users:
  - name: ${user}
    groups:
      - sudo
    shell: /bin/bash
    ssh_authorized_keys:
      - ${ssh_public_key}
    sudo: ALL=(ALL) NOPASSWD:ALL
runcmd:
  - apt update
  - apt-get install -y -q qemu-guest-agent net-tools
  - timedatectl set-timezone Europe/Brussels
  - systemctl enable qemu-guest-agent
  - systemctl start qemu-guest-agent

  - |
    export INSTALL_K3S_VERSION="${k3s_version}"
    export K3S_TOKEN="${token}"
    export INSTALL_K3S_EXEC="server ${k3s_args} --write-kubeconfig-mode 744 --tls-san ${tls_san} \
      --disable traefik --flannel-backend=none --disable-network-policy --disable-kube-proxy"
    curl -sfL https://get.k3s.io | sh -
  - export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  - until kubectl get pod; do sleep 5; done
  - kubectl label node $(hostname) node.kubernetes.io/worker=
  - |
    ln -sf /var/lib/rancher/k3s/agent/etc/containerd /etc/
    touch /etc/containerd/config.toml

    mkdir -p /etc/cni
    ln -sf /var/lib/rancher/k3s/agent/etc/cni/net.d /etc/cni/

    cat <<EOM >/etc/systemd/system/containerd.service
    [Unit]
    Description=Dummy Containerd Unit files

    [Service]
    Type=simple
    ExecStart=/usr/bin/sleep infinity
    EOM

    systemctl daemon-reload
    systemctl enable --now containerd
    systemctl status containerd

  - |
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  - |
    CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
    CLI_ARCH=amd64
    if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
    curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${"$"}{CILIUM_CLI_VERSION}/cilium-linux-${"$"}{CLI_ARCH}.tar.gz{,.sha256sum}
    sha256sum --check cilium-linux-${"$"}{CLI_ARCH}.tar.gz.sha256sum
    sudo tar xzvfC cilium-linux-${"$"}{CLI_ARCH}.tar.gz /usr/local/bin
    rm cilium-linux-${"$"}{CLI_ARCH}.tar.gz{,.sha256sum}



  - export HOME=/root
  - |
    set -x;
    cilium install --version 1.17.2 --wait --set=ipam.operator.clusterPoolIPv4PodCIDRList="10.42.0.0/16" \
      --set k8sServiceHost=${ip_address} \
      --set k8sServicePort=6443 \
      --set kubeProxyReplacement=true

  - |
    echo "Wait for cilium to be installed..."
    until kubectl rollout status -n kube-system deploy cilium-operator; do sleep 5; done

  - cilium hubble enable
  - cilium status
  - |
    HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
    HUBBLE_ARCH=amd64
    if [ "$(uname -m)" = "aarch64" ]; then HUBBLE_ARCH=arm64; fi
    curl -L --fail --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-${"$"}{HUBBLE_ARCH}.tar.gz{,.sha256sum}
    sha256sum --check hubble-linux-${"$"}{HUBBLE_ARCH}.tar.gz.sha256sum
    sudo tar xzvfC hubble-linux-${"$"}{HUBBLE_ARCH}.tar.gz /usr/local/bin
    rm hubble-linux-${"$"}{HUBBLE_ARCH}.tar.gz{,.sha256sum}

  - until kubectl get pod; do sleep 5; done
  - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  - |
    set -x;
    helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
        --version 4.12.0 --wait -n ingress-nginx --create-namespace \
        --set controller.config.force-ssl-redirect=true \
        --set controller.ingressClassResource.default=true

  - until kubectl rollout status -n ingress-nginx deploy ingress-nginx-controller; do sleep 5; done

  - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  - |
    helm upgrade --install --create-namespace -n monitoring kube-prometheus-stack prometheus-community/kube-prometheus-stack \
        --version 69.8.2

  - helm repo add argo https://argoproj.github.io/argo-helm
  - |
    set -x;
    helm upgrade --install --version 7.8.13 argocd argo/argo-cd --namespace argocd --wait --create-namespace \
      --set global.domain=argocd.k8s.ohmymndy.com \
      --set server.ingress.enabled=true \
      --set server.ingress.ingressClassName=nginx \
      --set configs.params."server\.insecure"=true \
      --set-json server.ingress.annotations={}


#  - |
#    kubectl apply -k github.com/confidential-containers/operator/config/release?ref=v0.12.0
#    kubectl apply -k github.com/confidential-containers/operator/config/samples/ccruntime/default?ref=v0.12.0

  - |
    apt-get install glusterfs-server nmap -y
    systemctl start glusterd

    mkdir -p /var/lib/gluster/gv0

    # wait for other nodes to be ready
    until nmap 10.0.40.82 -Pn  -p 24007 >/dev/null && nmap 10.0.40.83 -Pn  -p 24007 >/dev/null ; do sleep 5; done
    sleep 10
    gluster peer probe 10.0.40.82
    gluster peer probe 10.0.40.83


    gluster volume create gv0 replica 3 10.0.40.81:/var/lib/gluster/gv0 10.0.40.82:/var/lib/gluster/gv0 10.0.40.83:/var/lib/gluster/gv0 force
    gluster volume start gv0
    mkdir -p /mnt/gv0
    echo "127.0.0.1:/gv0 /mnt/gv0 glusterfs defaults,_netdev 0 0" >> /etc/fstab
    systemctl daemon-reload
    mount -a


  - echo "done" > /tmp/cloud-config.done